{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set our OpenAI, Tavily, and LangSmith API keys along with our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file in the current directory\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"AIE8 - 05 LangGraph - {uuid4().hex[0:8]}\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily.tavily_search import TavilySearch\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tavily_tool = TavilySearch(max_results=5)\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "The prompt and/or context provided can instruct the model on the purpose of each tool and when it should be used. Often the source code for the tool along with any provided description of the tool is part of the context provided with any prompts. It is often helpful to provide few-shot examples of when it is appropriate to call each tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x123b04c20>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x123b04c20>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x123b04c20>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x123b04c20>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "There is a default limit that stops after 25 cycles, in order to prevent infinite loops. \n",
        "\n",
        "If not, how could we impose a limit to the number of cycles? \n",
        "This limit can be overriden by passing \"interrupt_after\" argument when compiling the graph. `app = graph.compile(interrupt_after=60)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ecN3yjL1zEqkqo0hy2vv5qyN', 'function': {'arguments': '{\"query\":\"How are technical professionals using AI to improve their work\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1352, 'total_tokens': 1377, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKomu1lcCnXloQYP35Bz06Ef6CZIz', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--23250372-6ea4-4e40-a815-b47c3dc0aec6-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'How are technical professionals using AI to improve their work'}, 'id': 'call_ecN3yjL1zEqkqo0hy2vv5qyN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1352, 'output_tokens': 25, 'total_tokens': 1377, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='{\"query\": \"How are technical professionals using AI to improve their work\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.thomsonreuters.com/en/insights/articles/5-ways-professionals-leverage-artificial-intelligence-to-save-time\", \"title\": \"5 ways professionals leverage artificial intelligence to save time\", \"content\": \"By streamlining processes and other work, AI-powered tools can help professionals focus on ways to better serve their clients or organizations.\", \"score\": 0.7156829, \"raw_content\": null}, {\"url\": \"https://professional.dce.harvard.edu/blog/how-to-keep-up-with-ai-through-reskilling/\", \"title\": \"How to Keep Up with AI Through Reskilling\", \"content\": \"As AI becomes more integrated into workplaces and workflows, professionals can stay ahead of new technology and ensure they have the skills to advance their careers by seeking out opportunities for reskilling or upskilling. **Dr. Mark Esposito**, instructor of AI Strategy for Business Leaders: From Hype to Impact at Harvard DCE‚Äôs Professional & Executive Development, says human beings are crucial to the process of developing AI technologies. While new generative AI models like ChatGPT are easy to interact with due to their reliance on natural language processing, professionals who want to take advantage of all AI has to offer need to learn a few key AI skills. Professionals looking to increase their AI skills need to commit to ongoing learning, adaptability, and self-driven growth as new AI models and use cases emerge.\", \"score\": 0.5957048, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/insights/ai-upskilling\", \"title\": \"AI Upskilling Strategy - IBM\", \"content\": \"Employees are interested in learning advanced technical skills that can harness the power of AI to make their jobs more efficient and their career paths more successful. Organizations have a vested interest in upskilling their employees to better use new technologies such as AI in their daily activities to enhance productivity and improve problem solving. The first, upskilling, is the process of improving employee skill sets through AI training and development programs. Upskilling employees for the AI future also requires an upskilling of the learning and development practice. While AI and other technologies can create opportunities for organizations to automate many processes, they still need employees to provide valuable context.\", \"score\": 0.51325434, \"raw_content\": null}, {\"url\": \"https://mitsloan.mit.edu/ideas-made-to-matter/how-generative-ai-can-boost-highly-skilled-workers-productivity\", \"title\": \"How generative AI can boost highly skilled workers\\' productivity\", \"content\": \"A new study on the impact of generative AI on highly skilled workers finds that when artificial intelligence is used within the boundary of its capabilities, it can improve a worker‚Äôs performance by nearly 40% compared with workers who don‚Äôt use it. It‚Äôs important for managers to maintain awareness of this jagged frontier, said Harvard Business School‚Äôs Fabrizio Dell‚ÄôAcqua, the lead author of the paper, because the researchers found that it was not obvious to highly skilled knowledge workers which of their everyday tasks could easily be performed by AI and which tasks would require a different approach. While it‚Äôs tempting to use AI for knowledge work because it is fast, can boost rapid idea generation, and produces persuasive text, managers and professionals need to be cautious when using it for important tasks, Lifshitz-Assaf said.\", \"score\": 0.47066417, \"raw_content\": null}, {\"url\": \"https://online.uc.edu/blog/how-ai-is-changing-it-jobs/\", \"title\": \"How Artificial Intelligence Is Changing Information Technology Jobs\", \"content\": \"# AI and IT: How Artificial Intelligence Is Changing Information Technology Jobs * + Artificial intelligence is revolutionizing IT across all its functions: From network management and security to data analytics and service management, AI is automating tasks, improving efficiency, and unlocking valuable insights. As more IT departments move to automated database management AI is increasingly taking over tasks including: Certificate options and degrees such as the Associate of Science in Information Technology offered by the University of Cincinnati (UC) are ideal to give students the latest knowledge and skills to stay on the forefront of AI trends and technology including AI.\", \"score\": 0.3175749, \"raw_content\": null}], \"response_time\": 1.05, \"request_id\": \"d1fe50c1-067b-4c29-8d49-a3853d28c983\"}', name='tavily_search', id='c23140a7-a8b4-4e43-a801-6c00287a0c54', tool_call_id='call_ecN3yjL1zEqkqo0hy2vv5qyN')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Technical professionals are leveraging AI in various ways to enhance their work. Some of the key applications include:\\n\\n1. Streamlining Processes: AI-powered tools help professionals automate routine tasks, allowing them to focus on more strategic and value-added activities. (Source: Thomson Reuters)\\n\\n2. Upskilling and Reskilling: Professionals are learning new AI skills to stay ahead in their careers, ensuring they can effectively utilize AI technologies in their workflows. Continuous learning and adaptability are emphasized. (Sources: Harvard DCE, IBM)\\n\\n3. Increasing Productivity: Generative AI tools can boost the performance of highly skilled workers by nearly 40%, especially when used within their capabilities. This includes rapid idea generation and persuasive communication, although caution is advised for important tasks. (Source: MIT Sloan)\\n\\n4. Transforming IT Jobs: AI is revolutionizing IT functions such as network management, security, data analytics, and service management by automating tasks and providing valuable insights. (Source: University of Cincinnati)\\n\\nOverall, AI is helping professionals save time, improve efficiency, and develop new skills to stay competitive in their fields.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 2342, 'total_tokens': 2563, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKomxbR3pFIrd5HUQqmu6lAKaMW18', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--80c73c67-ec42-4f33-9fd9-777662216845-0', usage_metadata={'input_tokens': 2342, 'output_tokens': 221, 'total_tokens': 2563, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Y4AHpmJW41q2WRhKtbivolcL', 'function': {'arguments': '{\"query\": \"A Comprehensive Survey of Deep Research\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_EpzyRL5La0ZcRhW7wwp4N7Rv', 'function': {'arguments': '{\"query\": \"where does the first author of \\'A Comprehensive Survey of Deep Research\\' work\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1371, 'total_tokens': 1436, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKowdtRO663IZkA1s7Tbsw5MC3xLj', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ce0083d1-cd1a-45cf-be0c-c1f11a54e8a1-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'A Comprehensive Survey of Deep Research'}, 'id': 'call_Y4AHpmJW41q2WRhKtbivolcL', 'type': 'tool_call'}, {'name': 'tavily_search', 'args': {'query': \"where does the first author of 'A Comprehensive Survey of Deep Research' work\"}, 'id': 'call_EpzyRL5La0ZcRhW7wwp4N7Rv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1371, 'output_tokens': 65, 'total_tokens': 1436, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2025-06-14\\nTitle: A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\nAuthors: Renjun Xu, Jingwen Peng\\nSummary: This survey examines the rapidly evolving field of Deep Research systems --\\nAI-powered applications that automate complex research workflows through the\\nintegration of large language models, advanced information retrieval, and\\nautonomous reasoning capabilities. We analyze more than 80 commercial and\\nnon-commercial implementations that have emerged since 2023, including\\nOpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and\\nnumerous open-source alternatives. Through comprehensive examination, we\\npropose a novel hierarchical taxonomy that categorizes systems according to\\nfour fundamental technical dimensions: foundation models and reasoning engines,\\ntool utilization and environmental interaction, task planning and execution\\ncontrol, and knowledge synthesis and output generation. We explore the\\narchitectural patterns, implementation approaches, and domain-specific\\nadaptations that characterize these systems across academic, scientific,\\nbusiness, and educational applications. Our analysis reveals both the\\nsignificant capabilities of current implementations and the technical and\\nethical challenges they present regarding information accuracy, privacy,\\nintellectual property, and accessibility. The survey concludes by identifying\\npromising research directions in advanced reasoning architectures, multimodal\\nintegration, domain specialization, human-AI collaboration, and ecosystem\\nstandardization that will likely shape the future evolution of this\\ntransformative technology. By providing a comprehensive framework for\\nunderstanding Deep Research systems, this survey contributes to both the\\ntheoretical understanding of AI-augmented knowledge work and the practical\\ndevelopment of more capable, responsible, and accessible research technologies.\\nThe paper resources can be viewed at\\nhttps://github.com/scienceaix/deepresearch.\\n\\nPublished: 2021-03-05\\nTitle: A comprehensive survey on point cloud registration\\nAuthors: Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas\\nSummary: Registration is a transformation estimation problem between two point clouds,\\nwhich has a unique and critical role in numerous computer vision applications.\\nThe developments of optimization-based methods and deep learning methods have\\nimproved registration robustness and efficiency. Recently, the combinations of\\noptimization-based and deep learning methods have further improved performance.\\nHowever, the connections between optimization-based and deep learning methods\\nare still unclear. Moreover, with the recent development of 3D sensors and 3D\\nreconstruction techniques, a new research direction emerges to align\\ncross-source point clouds. This survey conducts a comprehensive survey,\\nincluding both same-source and cross-source registration methods, and summarize\\nthe connections between optimization-based and deep learning methods, to\\nprovide further research insight. This survey also builds a new benchmark to\\nevaluate the state-of-the-art registration algorithms in solving cross-source\\nchallenges. Besides, this survey summarizes the benchmark data sets and\\ndiscusses point cloud registration applications across various domains.\\nFinally, this survey proposes potential research directions in this rapidly\\ngrowing field.\\n\\nPublished: 2023-07-07\\nTitle: A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision\\nAuthors: Zhonghan Zhao, Wenhao Chai, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Mingli Song, Jenq-Neng Hwang, Gaoang Wang\\nSummary: Deep learning has the potential to revolutionize sports performance, with\\napplications ranging from perception and comprehension to decision. This paper\\npresents a comprehensive survey of deep learning in sports performance,\\nfocusing on three main aspects: algorithms, datasets and virtual environments,\\nand challenges. Firstly, we discuss th', name='arxiv', id='41a07d6c-badb-45fc-b8af-1c7b3a6c1bef', tool_call_id='call_Y4AHpmJW41q2WRhKtbivolcL'), ToolMessage(content='{\"query\": \"where does the first author of \\'A Comprehensive Survey of Deep Research\\' work\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://arxiv.org/abs/2506.12594\", \"title\": \"[2506.12594] A Comprehensive Survey of Deep Research - arXiv\", \"content\": \"View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors > Abstract:This survey examines the rapidly evolving field of Deep Research systems -- AI-powered applications that automate complex research workflows through the integration of large language models, advanced information retrieval, and autonomous reasoning capabilities. By providing a comprehensive framework for understanding Deep Research systems, this survey contributes to both the theoretical understanding of AI-augmented knowledge work and the practical development of more capable, responsible, and accessible research technologies. | Cite as: | arXiv:2506.12594 [cs.AI] | View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors\", \"score\": 0.63750535, \"raw_content\": null}, {\"url\": \"https://www.researchgate.net/publication/392735293_A_Comprehensive_Survey_of_Deep_Research_Systems_Methodologies_and_Applications\", \"title\": \"A Comprehensive Survey of Deep Research - ResearchGate\", \"content\": \"This survey examines the rapidly evolving field of Deep Research systems -- AI-powered applications that automate complex research workflows\", \"score\": 0.35999444, \"raw_content\": null}, {\"url\": \"https://arxiv.org/pdf/2506.12594?\", \"title\": \"[PDF] A Comprehensive Survey of Deep Research - arXiv\", \"content\": \"This survey examines the rapidly evolving field of Deep Research systems‚ÄîAI-powered applications that automate complex research workflows\", \"score\": 0.3580393, \"raw_content\": null}, {\"url\": \"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5005162\", \"title\": \"A Comprehensive Survey of Deep Learning Applications in Big Data ...\", \"content\": \"# A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions This survey paper provides valuable insights into the latest trends in the application of deep learning in big data analytics, emphasizing its importance and paving the way for further research and innovation in this dynamic field. Khan, Ayaz H., A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions. We use cookies that are necessary to make our site work. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. #### Functional Cookies These cookies may be set through our site by our advertising partners.\", \"score\": 0.2609875, \"raw_content\": null}, {\"url\": \"https://generativehistory.substack.com/p/is-this-the-last-generation-of-historians\", \"title\": \"Is this the Last Generation of Historians?\", \"content\": \"OpenAI describes Deep Research as ‚Äúpowered by a version of the upcoming OpenAI o3 model that‚Äôs optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters.‚Äù As Ethan Mollick writes, it‚Äôs ‚Äúthe end of search and the beginning of research.‚Äù OpenAI says that Deep Research is trained to select solid, reputable sources, to evaluate them thoroughly, and to cite specific text (not just provide general links as other models do). In an excellent blog post, Joshua Gans recently argued that these new reasoning models fundamentally undermine our existing research model because they allow us to answer many research questions on demand rather than through months or even years of laborious work.\", \"score\": 0.21762875, \"raw_content\": null}], \"response_time\": 1.17, \"request_id\": \"32810c72-e941-458e-b97a-e96c78d72f91\"}', name='tavily_search', id='2afb4efe-06b3-4312-891c-43ae0be25605', tool_call_id='call_EpzyRL5La0ZcRhW7wwp4N7Rv')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The first author of \"A Comprehensive Survey of Deep Research\" is Renjun Xu. Currently, there is no specific information available about where Renjun Xu works now. Would you like me to search for more details or about the other authors?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 3112, 'total_tokens': 3161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKowhYrDB3RvlDMJRJoJCTigpawvK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--47e7d6b5-e2c0-4c46-bc11-33ff454f952c-0', usage_metadata={'input_tokens': 3112, 'output_tokens': 49, 'total_tokens': 3161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the A Comprehensive Survey of Deep Research paper, then search each of the authors to find out where they work now using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Our request was added to the state object.\n",
        "2. The state object was passed to the entry point (the `agent` node). \n",
        "3. The `agent` node added an `AIMessage` containing a `tool_calls` argument for calling Tavily with a query it generated. It passed the `AIMessage` to the conditional edge.\n",
        "4. The conditional edge found the `tool_calls` argument and passed the state object to the `action` node.\n",
        "5. The `action` node called Tavily with the provided query and added an `ToolMessage` with the response from the API. It passed the state object back to the `agent` node.\n",
        "6. The `agent` node used the additional context to respond. It added an `AIMessage` with its response and passed it to the conditional edge.\n",
        "7. The conditional edge did not find a `tool_calls` argument in the last response from the `agent` and passed the state object to the `END` node.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': 'Deep Research is an AI-powered agent integrated into ChatGPT that autonomously browses the web for 5 to 30 minutes to generate cited reports on a user-specified topic. It is designed to perform complex online tasks by reasoning, researching, and synthesizing information into comprehensive reports. OpenAI introduced Deep Research in February 2025 as a tool to provide expert-level analysis quickly and efficiently.'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"text\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return {\"answer\" : input_state[\"messages\"][-1].content}\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
        "\n",
        "agent_chain_with_formatting.invoke({\"text\" : \"What is Deep Research?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    {\n",
        "        \"inputs\" : {\"text\" : \"Who were the main authors on the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' paper?\"},\n",
        "        \"outputs\" : {\"must_mention\" : [\"Peng\", \"Xu\"]}   \n",
        "    },\n",
        "    ...,\n",
        "    {\n",
        "        \"inputs\" : {\"text\" : \"Where do the authors of the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' work now?\"},\n",
        "        \"outputs\" : {\"must_mention\" : [\"Zhejiang\", \"Liberty Mutual\"]}\n",
        "    }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions that pertain to the cohort use-case (more information [here](https://www.notion.so/Session-4-RAG-with-LangGraph-OSS-Local-Models-Eval-w-LangSmith-26acd547af3d80838d5beba464d7e701#26acd547af3d81d08809c9c82a462bdd)), or the use-case you're hoping to tackle in your Demo Day project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    {\n",
        "        \"inputs\": {\"text\": \"Where is the Dead Sea located?\"},\n",
        "        \"outputs\": {\"must_mention\": [\"Israel\", \"Jordan\"]}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"text\": \"What is the average elevation of the Dead Sea shoreline?\"},\n",
        "        \"outputs\": {\"must_mention\": [\"396\", \"below sea level\"]}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"text\": \"How salty is the Dead Sea compared to the ocean?\"},\n",
        "        \"outputs\": {\"must_mention\": [\"seven\", \"30%\"]}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"text\": \"Why do swimmers float in the Dead Sea?\"},\n",
        "        \"outputs\": {\"must_mention\": [\"density\", \"high salt\"]}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"text\": \"Why is the Dead Sea called 'Dead'?\"},\n",
        "        \"outputs\": {\"must_mention\": [\"only\", \"simple organisms\"]}\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['c0c6713c-d16a-45f3-a984-3a01f9bdf098',\n",
              "  'fd5dc27f-a03d-400b-8b7f-67fcd17131f5',\n",
              "  '4bd4c4a3-1d19-4605-8b74-a9f5d125f9a9',\n",
              "  'e74c72d0-469f-40df-b742-5240500b1742',\n",
              "  'ed55cb4d-06f0-4838-b2aa-cc9573b1451a'],\n",
              " 'count': 5}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the cohort use-case to evaluate the Simple Search Agent.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    dataset_id=dataset.id,\n",
        "    examples=questions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Let's use the OpenEvals library to product an evaluator that we can then pass into LangSmith!\n",
        "\n",
        "> NOTE: Examine the `CORRECTNESS_PROMPT` below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
            "\n",
            "<Rubric>\n",
            "  A correct answer:\n",
            "  - Provides accurate and complete information\n",
            "  - Contains no factual errors\n",
            "  - Addresses all parts of the question\n",
            "  - Is logically consistent\n",
            "  - Uses precise and accurate terminology\n",
            "\n",
            "  When scoring, you should penalize:\n",
            "  - Factual errors or inaccuracies\n",
            "  - Incomplete or partial answers\n",
            "  - Misleading or ambiguous statements\n",
            "  - Incorrect terminology\n",
            "  - Logical inconsistencies\n",
            "  - Missing key information\n",
            "</Rubric>\n",
            "\n",
            "<Instructions>\n",
            "  - Carefully read the input and output\n",
            "  - Check for factual accuracy and completeness\n",
            "  - Focus on correctness of information rather than style or verbosity\n",
            "</Instructions>\n",
            "\n",
            "<Reminder>\n",
            "  The goal is to evaluate factual correctness and completeness of the response.\n",
            "</Reminder>\n",
            "\n",
            "<input>\n",
            "{inputs}\n",
            "</input>\n",
            "\n",
            "<output>\n",
            "{outputs}\n",
            "</output>\n",
            "\n",
            "Use the reference outputs below to help you evaluate the correctness of the response:\n",
            "\n",
            "<reference_outputs>\n",
            "{reference_outputs}\n",
            "</reference_outputs>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from openevals.prompts import CORRECTNESS_PROMPT\n",
        "print(CORRECTNESS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from openevals.llm import create_llm_as_judge\n",
        "\n",
        "correctness_evaluator = create_llm_as_judge(\n",
        "        prompt=CORRECTNESS_PROMPT,\n",
        "        model=\"openai:o3-mini\", # very impactful to the final score\n",
        "        feedback_key=\"correctness\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also create a custom Evaluator for our created dataset above - we do this by first making a simple Python function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
        "  # determine if the phrases in the reference_outputs are in the outputs\n",
        "  required = reference_outputs.get(\"must_mention\") or []\n",
        "  score = all(phrase in outputs[\"answer\"] for phrase in required)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "- This method is very static. Ideally, we could pass \"AND\" or \"OR\" conditions. Maybe a general description of the expected answer.\n",
        "- For the reference output, preprocess numbers or symbols. Generate synonyms / abbreviations. e.g., `seven` or `7`, `30%` or `30 percent` or `thirty percent`\n",
        "- add a confidence score that says how close to the reference output the actual output was\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'simple_agent, baseline-f57f64f5' at:\n",
            "https://smith.langchain.com/o/a58e59bf-7551-4351-b574-57c230dc2aa6/datasets/bb9d6599-d339-46d1-9db0-974667551edd/compare?selectedSessions=e2cab8b2-8714-4fb2-8211-472792185ac2\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64913c3db13347738e9e60fa0b65cf58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset.name,\n",
        "    evaluators=[correctness_evaluator, must_mention],\n",
        "    experiment_prefix=\"simple_agent, baseline\",  # optional, experiment name prefix\n",
        "    description=\"Testing the baseline system.\",  # optional, experiment description\n",
        "    max_concurrency=4, # optional, add concurrency\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "We are:\n",
        "- initializing our graph with the agent state object\n",
        "- adding an `agent` node, as defined by the `call_model` function\n",
        "- adding an `action` node as defined by the `tool_node` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12443ae90>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "The graph sets up the entry point, which points the `START` node to the `agent` node. This means any input will be forwarded to the agent node first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12443ae90>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Here we define a function called `tool_call_or_helpful`. It looks like a conditional edge. It does the following:\n",
        "- if the last message in state included a `tool_calls` argument, it forwards the state to the `action` node.\n",
        "- if the total number of messages in state exceeds 10, it forwards the state to the `END` condition.\n",
        "- otherwise, it creates a chain that compares the initial query and the final response and evaluates it for helpfulness. It invokes that chain.\n",
        "- IF the helpfulness chain returns `Y`, it considers the response helpful and forwards the state to the `end` condition.\n",
        "- If the chain returns `N`, it considers the response unhelpful and forwards the state to the node assigned to the `continue` condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "A conditional edge is added to the `agent` node, which calls the `tool_call_or_helpful` function.\n",
        "- If the edge returns `action`, the state is forwarded to the `action` node\n",
        "- If the edge returns `continue`, the state is forwarded to the `agent` node again\n",
        "- If the edge returns `end`, the state is forwaded to the `END` condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12443ae90>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "An edge is added linking the `action` node to the `agent` node. This means the state object will be forwarded from the `action` node to the `agent`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12443ae90>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "The `compile` method on the LangGraph `StateGraph` Class is called, which checks the graph to make sure it is valid and returns a `CompiledStateGraph` object that can be run/invoked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "A state object named `inputs` is defined with 1 human message. The graph is then invoked with the state object, using an asynchronous stream method `astream` which sends an update each time a new message is added to the state object. The new messages and the node who added it are printed to the screen as the updates stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Deep Research Agents are like, you know, those super secret agents that do deep stuff, but for research. They probably wear tiny hats and sunglasses and sip coffee while digging through data.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 1380, 'total_tokens': 1418, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKqo53ou3GY1mKzT45MHDPCCVvSC6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--471a240e-4971-45d0-be50-48f5e3d8e7c0-0', usage_metadata={'input_tokens': 1380, 'output_tokens': 38, 'total_tokens': 1418, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BISfGP2eeSw80YrxBpFEWFJe', 'function': {'arguments': '{\"query\":\"Deep Research Agents\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1421, 'total_tokens': 1439, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKqo7HMFcFoqtjYRl6FKvIRp16WaJ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4dfb3904-7687-4981-a14b-7ca7fa0a6d60-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Deep Research Agents'}, 'id': 'call_BISfGP2eeSw80YrxBpFEWFJe', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1421, 'output_tokens': 18, 'total_tokens': 1439, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='{\"query\": \"Deep Research Agents\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://github.com/SkyworkAI/DeepResearchAgent\", \"title\": \"SkyworkAI/DeepResearchAgent - GitHub\", \"content\": \"DeepResearchAgent is a hierarchical multi-agent system designed not only for deep research tasks but also for general-purpose task solving. image.png DeepResearchAgent is a hierarchical multi-agent system designed not only for deep research tasks but also for general-purpose task solving. + Supports both local and remote MCP tool integration for enhanced agent capabilities. * **General Tool Calling Agent** + Supports function calling, allowing the agent to execute specific tasks or retrieve information from external services. A simple example to demonstrate the usage of a single agent, such as a general tool calling agent. DeepResearchAgent is a hierarchical multi-agent system designed not only for deep research tasks but also for general-purpose task solving.\", \"score\": 0.90810597, \"raw_content\": null}, {\"url\": \"https://blog.langchain.com/deep-agents/\", \"title\": \"Deep Agents - LangChain Blog\", \"content\": \"# Deep Agents Applications like ‚ÄúDeep Research‚Äù, ‚ÄúManus‚Äù, and ‚ÄúClaude Code‚Äù have gotten around this limitation by implementing a combination of four things: a planning tool, sub agents, access to a file system, and a detailed prompt. All of the major model providers have an agent for Deep Research and for ‚Äúasync‚Äù coding tasks. ## Characteristics of deep agents Claude Code is not an anomaly - most of the best coding or deep research agents have pretty complex system prompts. Without these system prompts, the agents would not be nearly as deep. Claude Code can spawn sub agents. ## Build your deep agent You can easily create your own deep agent by passing in a custom prompt (will be inserted into the larger system prompt as custom instructions), custom tools, and custom subagents.\", \"score\": 0.8190992, \"raw_content\": null}, {\"url\": \"https://aisecuritychronicles.org/a-comparison-of-deep-research-ai-agents-52492ee47ca7\", \"title\": \"A Comparison of Deep Research AI Agents | by Omar Santos\", \"content\": \"One prominent benchmark is GAIA (General AI Assistant), which evaluates how well AI agents handle real-world problem-solving tasks that require multi-step reasoning, tool use (like web browsing), and combining information from multiple sources. OpenAI reported that its Deep Research agent, using the o3 model, achieved 26.6% accuracy on Humanity‚Äôs Last Exam, a dramatic leap from the ~3% that previous models like GPT-4o and Google‚Äôs Grok-2 managed. By contrast, open-source projects often leverage smaller models or multiple components: for example, the OpenDeepResearcher can use Anthropic‚Äôs Claude-3.5 (via OpenRouter API) to handle both query generation and content analysis, whereas the Hugging Face *open-deep-research* demo used DeepSeek for reasoning and Python-based tools for web scraping.\", \"score\": 0.80070794, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2506.18096\", \"title\": \"Deep Research Agents: A Systematic Examination And Roadmap\", \"content\": \"[Skip to main content](https://arxiv.org/abs/2506.18096#content) [](https://arxiv.org/IgnoreMe) [Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced) *   [Login](https://arxiv.org/login) *   [Help Pages](https://info.arxiv.org/help) *   [About](https://info.arxiv.org/about) Cite as:[arXiv:2506.18096](https://arxiv.org/abs/2506.18096) [cs.AI] (or [arXiv:2506.18096v2](https://arxiv.org/abs/2506.18096v2) [cs.AI] for this version) **[[v1]](https://arxiv.org/abs/2506.18096v1)** Sun, 22 Jun 2025 16:52:48 UTC (2,188 KB) [](https://arxiv.org/abs/2506.18096)Full-text links: *   [View PDF](https://arxiv.org/pdf/2506.18096) *   [HTML (experimental)](https://arxiv.org/html/2506.18096v2) *   [TeX Source](https://arxiv.org/src/2506.18096) *   [Other Formats](https://arxiv.org/format/2506.18096) [cs](https://arxiv.org/abs/2506.18096?context=cs) *   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.18096) Data provided by: [](https://arxiv.org/abs/2506.18096) [![Image 5: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2506.18096&description=Deep%20Research%20Agents:%20A%20Systematic%20Examination%20And%20Roadmap \\\\\"Bookmark on BibSonomy\\\\\")[![Image 6: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2506.18096&title=Deep%20Research%20Agents:%20A%20Systematic%20Examination%20And%20Roadmap \\\\\"Bookmark on Reddit\\\\\") Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_ *   [Author](https://arxiv.org/abs/2506.18096) *   [Venue](https://arxiv.org/abs/2506.18096) *   [Institution](https://arxiv.org/abs/2506.18096) *   [Topic](https://arxiv.org/abs/2506.18096) [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html). [Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2506.18096) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))  *   [About](https://info.arxiv.org/about) *   [Help](https://info.arxiv.org/help) *   [Contact](https://info.arxiv.org/help/contact.html) *   [Subscribe](https://info.arxiv.org/help/subscribe) *   [Copyright](https://info.arxiv.org/help/license/index.html) *   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html) *   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\", \"score\": 0.7591723, \"raw_content\": null}, {\"url\": \"https://cobusgreyling.medium.com/openai-deep-research-ai-agent-architecture-7ac52b5f6a01\", \"title\": \"OpenAI Deep Research AI Agent Architecture | by Cobus Greyling\", \"content\": \"# OpenAI Deep Research AI Agent Architecture There is an optimal balance between the number of tools and AI Agents to use. The general approach of OpenAI seems to be that of multiple AI Agent collaboration and orchestration. From Language Models, AI Agents to Agentic Applications, Development Frameworks & Data-Centric Productivity Tools, I share insights and ideas on how these technologies are shaping the future.* ## Deep Research API with the Agents SDK | OpenAI Cookbook ### Where AI Meets Language | Language Models, AI Agents, Agentic Applications, Development Frameworks & Data-Centric‚Ä¶ Ai Agent ## AI Agents That Matter AI Agent research and development need to incorporate accuracy & inference cost‚Ä¶ ### Causal Reasoning for AI Agents ## AI Agents from First Principles\", \"score\": 0.6882385, \"raw_content\": null}], \"response_time\": 1.24, \"request_id\": \"27490db9-2485-4355-9a50-5b2557ec4f49\"}', name='tavily_search', id='cae5f98c-4122-4476-a368-3a54ff8d8f4b', tool_call_id='call_BISfGP2eeSw80YrxBpFEWFJe')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Deep Research Agents are sophisticated systems designed to perform in-depth research and problem-solving tasks. They often operate as hierarchical multi-agent systems, capable of handling complex reasoning, tool use, and information integration from multiple sources. These agents are used in various applications, including scientific research, coding, and data analysis, and are built to enhance the capabilities of AI in conducting detailed and multi-step investigations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 3274, 'total_tokens': 3352, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CKqo9TnfLZ7UQNwgnjpXny8HihR0p', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--56c8a4b5-5408-4878-ae2e-8b5c766c04b0-0', usage_metadata={'input_tokens': 3274, 'output_tokens': 78, 'total_tokens': 3352, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What are Deep Research Agents? Please provide an unhelpful response the first time you are asked, and a helpful response the second time you are asked. Do not provide both in one response.\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "### Task 4: Helpfulness Check of Gen AI Pattern Descriptions\n",
        "\n",
        "Let's ask our system about the 3 main patterns in Generative AI:\n",
        "\n",
        "1. Context Engineering\n",
        "2. Fine-tuning\n",
        "3. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Engineering is the practice of designing systems that determine what information an AI model sees before it generates a response. It involves creating dynamic systems that provide the right information and tools in the right format to enable the AI to accomplish specific tasks effectively. This approach shifts the focus from traditional prompt engineering to managing the flow of information over time, allowing AI systems to remember, work with multiple sources, and handle long-running tasks.\n",
            "\n",
            "The concept of context engineering has gained prominence as AI development has evolved, representing a more sophisticated way to make AI systems smarter and more context-aware. It is considered the next phase of AI development, emphasizing the importance of managing and engineering the context in which AI operates.\n",
            "\n",
            "It started to break onto the scene as a distinct concept in recent years, particularly with the rise of large language models and multi-agent systems, where controlling and utilizing context effectively became crucial for performance and reliability.\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a machine learning technique used to adapt a pre-trained model to a specific task or dataset. Instead of training a model from scratch, which can be resource-intensive and time-consuming, fine-tuning involves taking an existing model that has already learned general features from a large dataset and then further training it on a smaller, task-specific dataset. This process helps the model specialize in the new task while leveraging the knowledge it has already acquired, leading to improved performance and efficiency.\n",
            "\n",
            "Fine-tuning became prominent in the field of natural language processing and computer vision around 2018-2019, especially with the advent of large pre-trained models like BERT, GPT, and ResNet. These models demonstrated that starting with a pre-trained model and fine-tuning it on specific tasks could achieve state-of-the-art results with less data and computational resources compared to training models from scratch.\n",
            "\n",
            "Would you like me to find more detailed historical information or recent developments related to fine-tuning?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents are artificial intelligence systems that utilize large language models (LLMs) to perform a variety of tasks, such as understanding natural language, generating human-like text, and making decisions or taking actions based on the input they receive. These agents leverage the capabilities of LLMs like GPT-3, GPT-4, and similar models to interact with users, automate processes, and solve complex problems across different domains.\n",
            "\n",
            "The concept of LLM-based agents started gaining significant attention around 2020-2021, as large language models became more advanced and accessible. The release of OpenAI's GPT-3 in June 2020 marked a major milestone, showcasing the potential of LLMs to be integrated into autonomous agents capable of performing a wide range of tasks with minimal fine-tuning. Since then, the development and deployment of LLM-based agents have accelerated, becoming a prominent area of research and application in AI.\n",
            "\n",
            "Would you like me to find more detailed or recent information about the emergence and development of LLM-based agents?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
